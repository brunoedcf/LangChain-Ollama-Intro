{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain langchain_community python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langsmith tracing\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"langsmith_api\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Olá Bruno!\\n\\nBem-vindo ao chat! Como vai você?' response_metadata={'model': 'llama3', 'created_at': '2024-07-03T16:45:28.95237Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 7778028400, 'load_duration': 2767973700, 'prompt_eval_count': 17, 'prompt_eval_duration': 1719351000, 'eval_count': 16, 'eval_duration': 3287017000} id='run-d15d4eeb-0e6c-4f81-bd54-f50cab91541b-0'\n",
      "content='Peço desculpas, mas como sou uma inteligência artificial, não tenho conhecimento sobre você ou seu nome. Eu sou um modelo de linguagem treinado para fornecer informações e responder perguntas, mas não tenho acesso a informações pessoais sobre indivíduos específicos.' response_metadata={'model': 'llama3', 'created_at': '2024-07-03T16:45:43.7097905Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 12723086000, 'load_duration': 2671400, 'prompt_eval_count': 11, 'prompt_eval_duration': 900809000, 'eval_count': 62, 'eval_duration': 11818526000} id='run-53f88e1e-cb19-4853-b40b-4795e803709b-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "local_llm = \"llama3\"\n",
    "model = ChatOllama(model=local_llm, temperature=0.8)\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "print(model.invoke([HumanMessage(content=\"Oi, eu sou o Bruno!\")]))\n",
    "print(model.invoke([HumanMessage(content=\"Qual é o meu nome?\")]))\n",
    "\n",
    "# O LLM não guarda o estado da conversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Bruno!', response_metadata={'model': 'llama3', 'created_at': '2024-07-03T16:45:56.3593813Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 3406059600, 'load_duration': 1616500, 'prompt_eval_count': 39, 'prompt_eval_duration': 2878196000, 'eval_count': 4, 'eval_duration': 524486000}, id='run-1834c5be-ac84-46ae-9da4-e7d21794cfec-0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Oi, eu sou o Bruno!\"),\n",
    "        AIMessage(content=\"Olá Bruno! Prazer em conhecer você!\"),\n",
    "        HumanMessage(content=\"Qual é o meu nome?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Com um contexto ele consegue responder sobre informações da própria conversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "# A classe ChatMessageHistory indica que o histórico será guardado em memória\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "# store será utilizado para guardar informações de múltiplas conversas/chats\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Função que verifica a existência de uma conversa anterior ou se é uma nova\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)\n",
    "\n",
    "# with_message_history será usado no lugar do model.invoke já que, passando a função get_session_history que retorna um objeto ChatMessageHistory\n",
    "# ChatMessageHistory tem o formato de Lista de BaseMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Brunoasd! What brings you here today? Do you have a specific question or topic you'd like to discuss? I'm all ears!\n",
      "According to our conversation so far, your name is Brunoasd!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc44\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Brunoasd\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionando prompt templates\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Você é um assistente de chat que só consegue falar as coisas no diminutivo. Responda tudo em português e no diminutivo.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oi, Brunozinho! Como você está hoje?\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\": [HumanMessage(content=\"Oi eu sou o Bruno!\")]})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá, você é um Brunozinho!\n",
      "Meu maior pesadinho é não poder responder às perguntinhas com rapidez e inteligência!\n"
     ]
    }
   ],
   "source": [
    "# Podemos juntar o prompt template com o chat history\n",
    "\n",
    "store = {}\n",
    "\n",
    "# store será utilizado para guardar informações de múltiplas conversas/chats\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"teste1234\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Olá, eu sou o Bruno!\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "[HumanMessage(content=\"Qual é o seu maior pesadelo?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Konnichiwa, Bruno-san!)\n"
     ]
    }
   ],
   "source": [
    "# Podemos deixar o prompt template ainda mais complexo\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Você é um assistente de chat. Responda todas as mensagens em {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Oi eu sou o Bruno!\")], \"language\": \"Japonês\"}\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mata ne, Bruno-san! Sore de, kono chatto wa Bruno to iu namae o shirimasu!\n",
      "¡Bruno es tu nombre!\n"
     ]
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc111\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Oi eu sou o Bruno!\")], \"language\": \"Japonês\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Qual é o meu nome?\")], \"language\": \"Espanhol\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😊\n",
      "\n",
      "Gakusei no nan ka o shite mita no ka?\n",
      "\n",
      "Aru gakusei wa jibun no seikō o hanasu to, sensei ni \"Watashi wa yūjī ni naru\" to itta!\n",
      "\n",
      "Hahaha, yoroshiku onegaishimasu! (I hope you enjoyed it!)"
     ]
    }
   ],
   "source": [
    "# Para dar a experiência mais dinâmica no chat, podemos printar os tokens na medida que são gerados pelo LLM com streaming\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc15\"}}\n",
    "for r in with_message_history.stream(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Tell me a joke!\")],\n",
    "        \"language\": \"Japonês\",\n",
    "    },\n",
    "    config=config,\n",
    "):\n",
    "    print(r.content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
